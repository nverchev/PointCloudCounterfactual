defaults:
    - scheduler: cosine
    - _self_

optimizer_name: AdamW
learning_rate: 0.004
gradient_op:
opt_settings:
    weight_decay: 0.001